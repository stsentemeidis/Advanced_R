\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={House Prices Seattle Individual Assignment},
            pdfauthor={Stavros Tsentemeidis},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{House Prices Seattle Individual Assignment}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Stavros Tsentemeidis}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{5/13/2019}


\begin{document}
\maketitle

\hypertarget{github-repository}{%
\subsection{GitHub Repository}\label{github-repository}}

The whole project is under this repository at my GitHub account.\\
\textbf{\url{https://github.com/stsentemeidis/House_Prices_Seattle}}

\hypertarget{loading-packages-data}{%
\subsection{Loading packages \& Data}\label{loading-packages-data}}

Before starting our EDA and model development pipeline, we first need to
install and load:

\begin{itemize}
\tightlist
\item
  The necessary \textbf{packages}
\item
  The data file named \textbf{house prices } we are about to use
\item
  The custom \textbf{functions} that have been created to make the main
  script more clear, interpreatable and efficient.
\end{itemize}

\hypertarget{explanation-summary-of-the-dataset}{%
\subsection{Explanation \& summary of the
dataset}\label{explanation-summary-of-the-dataset}}

To begin with let's have a look at the meaning of the initial variables
provided for the dataset.

\begin{itemize}
\tightlist
\item
  \textbf{id}: a notation for a house\\
\item
  \textbf{date}: date house was sold\\
\item
  \textbf{price}: price is prediction target\\
\item
  \textbf{bedrooms}: number of bedrooms per house\\
\item
  \textbf{bathrooms}: number of bathrooms per house\\
\item
  \textbf{sqft\_living}: square footage of the home\\
\item
  \textbf{sqft\_lot}: square footage of the lot\\
\item
  \textbf{floors}: total floors (levels) in house\\
\item
  \textbf{waterfront}: house which has a view to a waterfront\\
\item
  \textbf{view}: has been viewed\\
\item
  \textbf{condition}: how good the condition is (overall)\\
\item
  \textbf{grade}: overall grade given to the housing unit, based on King
  County grading system\\
\item
  \textbf{sqft\_above}: square footage of house apart from basement\\
\item
  \textbf{sqft\_basement}: square footage of the basement\\
\item
  \textbf{yr\_built}: built year
\item
  \textbf{yr\_renovated}: year when house was renovated\\
\item
  \textbf{zipcode}: zip of a house\\
\item
  \textbf{lat}: latitude coordinate\\
\item
  \textbf{long}: longtitude coordinate\\
\item
  \textbf{sqft\_living15}: living room are in 2015 (implies some
  renovations). This might or might not have affected the lotsize area\\
\item
  \textbf{sqft\_lot15}: lotsize area in 2015 (implies some renovations)
\end{itemize}

However, as we can see in the below correlation plots, the variable
\textbf{view} has a high correlation with \textbf{waterfront}. Adding to
that the variable \textbf{view} has many zeros. These 2 things lead me
to the conclusion that variable view is not the amount of times viewed
before sold, but \textbf{whether the view is good or not.}

From the below summary and description we can see that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are no \textbf{missing values}, so our dataset is complete.
\item
  The variables \emph{condition} and \emph{grade} are ordinal variables.
\item
  The variables above need to be converted from int to \emph{factors}.
\item
  \emph{Bedrooms}, \emph{bathrooms} and \emph{floors} are variables with
  discrete levels that can be converted to factors probably later on..
\end{enumerate}

\begin{verbatim}
## 'data.frame':    17277 obs. of  21 variables:
##  $ id           : num  9.18e+09 4.64e+08 2.22e+09 6.16e+09 6.39e+09 ...
##  $ date         : Factor w/ 372 levels "1/10/2015","1/12/2015",..: 211 331 290 20 226 223 94 34 76 294 ...
##  $ price        : num  225000 641250 810000 330000 530000 ...
##  $ bedrooms     : int  3 3 4 4 4 4 4 3 4 3 ...
##  $ bathrooms    : num  1.5 2.5 3.5 1.5 1.75 3.5 3.25 2.25 2.5 1.5 ...
##  $ sqft_living  : int  1250 2220 3980 1890 1814 3120 4160 1440 2250 2540 ...
##  $ sqft_lot     : int  7500 2550 209523 7540 5000 5086 47480 10500 6840 9520 ...
##  $ floors       : num  1 3 2 1 1 2 2 1 2 1 ...
##  $ waterfront   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ view         : int  0 2 2 0 0 0 0 0 0 0 ...
##  $ condition    : int  3 3 3 4 4 3 3 3 3 3 ...
##  $ grade        : int  7 10 9 7 7 9 10 8 9 8 ...
##  $ sqft_above   : int  1250 2220 3980 1890 944 2480 4160 1130 2250 1500 ...
##  $ sqft_basement: int  0 0 0 0 870 640 0 310 0 1040 ...
##  $ yr_built     : int  1967 1990 2006 1967 1951 2008 1995 1983 1987 1959 ...
##  $ yr_renovated : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ zipcode      : int  98030 98117 98024 98155 98115 98115 98072 98023 98058 98115 ...
##  $ lat          : num  47.4 47.7 47.6 47.8 47.7 ...
##  $ long         : num  -122 -122 -122 -122 -122 ...
##  $ sqft_living15: int  1260 2200 2220 1890 1290 1880 3400 1510 2480 1870 ...
##  $ sqft_lot15   : int  7563 5610 65775 8515 5000 5092 40428 8125 7386 6800 ...
\end{verbatim}

id

date

price

bedrooms

bathrooms

sqft\_living

sqft\_lot

floors

waterfront

view

condition

grade

sqft\_above

sqft\_basement

yr\_built

yr\_renovated

zipcode

lat

long

sqft\_living15

sqft\_lot15

Min. :1.00e+06

6/23/2014: 118

Min. : 78000

Min. : 1.00

Min. :0.50

Min. : 370

Min. : 520

Min. :1.00

Min. :0.0000

Min. :0.000

Min. :1.00

Min. : 3.00

Min. : 370

Min. : 0

Min. :1900

Min. : 0.0

Min. :98001

Min. :47.2

Min. :-123

Min. : 460

Min. : 659

1st Qu.:2.11e+09

6/25/2014: 113

1st Qu.: 320000

1st Qu.: 3.00

1st Qu.:1.75

1st Qu.: 1430

1st Qu.: 5050

1st Qu.:1.00

1st Qu.:0.0000

1st Qu.:0.000

1st Qu.:3.00

1st Qu.: 7.00

1st Qu.:1190

1st Qu.: 0

1st Qu.:1951

1st Qu.: 0.0

1st Qu.:98033

1st Qu.:47.5

1st Qu.:-122

1st Qu.:1490

1st Qu.: 5100

Median :3.90e+09

6/26/2014: 110

Median : 450000

Median : 3.00

Median :2.25

Median : 1910

Median : 7620

Median :1.50

Median :0.0000

Median :0.000

Median :3.00

Median : 7.00

Median :1564

Median : 0

Median :1975

Median : 0.0

Median :98065

Median :47.6

Median :-122

Median :1840

Median : 7639

Mean :4.57e+09

4/22/2015: 108

Mean : 539865

Mean : 3.37

Mean :2.11

Mean : 2080

Mean : 15186

Mean :1.49

Mean :0.0075

Mean :0.233

Mean :3.41

Mean : 7.66

Mean :1791

Mean : 289

Mean :1971

Mean : 85.4

Mean :98078

Mean :47.6

Mean :-122

Mean :1986

Mean : 12826

3rd Qu.:7.30e+09

7/8/2014 : 104

3rd Qu.: 645500

3rd Qu.: 4.00

3rd Qu.:2.50

3rd Qu.: 2550

3rd Qu.: 10695

3rd Qu.:2.00

3rd Qu.:0.0000

3rd Qu.:0.000

3rd Qu.:4.00

3rd Qu.: 8.00

3rd Qu.:2210

3rd Qu.: 556

3rd Qu.:1997

3rd Qu.: 0.0

3rd Qu.:98117

3rd Qu.:47.7

3rd Qu.:-122

3rd Qu.:2360

3rd Qu.: 10080

Max. :9.90e+09

4/14/2015: 102

Max. :7700000

Max. :33.00

Max. :8.00

Max. :13540

Max. :1164794

Max. :3.50

Max. :1.0000

Max. :4.000

Max. :5.00

Max. :13.00

Max. :9410

Max. :4820

Max. :2015

Max. :2015.0

Max. :98199

Max. :47.8

Max. :-121

Max. :6210

Max. :871200

NA

(Other) :16622

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

At this point it is decided:

\begin{itemize}
\tightlist
\item
  to keep \emph{numeric} as the common data type
\item
  change the \emph{dates} to a universal date type format of
  ``\%m/\%d/\%Y''
\item
  create the column \emph{price} for the test set as it is missing
\item
  subset the numerical data columns in order to plot some initial EDA.
\end{itemize}

As a first stage approach, the initial \textbf{correlation matrix} is
plotted. Based on that we can notice that:

\begin{itemize}
\tightlist
\item
  \textbf{Price} is highly correlated with the \textbf{sqft\_living}
  variables, something that we would expect from intuition.
\item
  \textbf{Sqft\_living} is highly correlated with the
  \textbf{sqft\_above}
\end{itemize}

Furthermore, in order to understand our target variable better, the
histogram of the prices is plotted. Referring to that we notice that the
distribution of the prices is heavily right skewed, which makes sense as
there are few really expensive ones.

In order to help our models predictability and performance, at this
stage it is decided to apply the \textbf{log} transformation on the
target column, but also make sure we undo that after the predictions by
applying the \textbf{exp} function.

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Correlation 1-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Histograms of Prices-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Relationship of Sqft_living & Price-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Relationship of Sqft_living & Sqft_above-1} \end{center}

 After having some first point of views on our dataset, it is time to
take advantage of the coordinates given for the houses sold. By using
the \textbf{Google Maps API} through the \textbf{ggmap} of ggplot2 in R,
we depict different maps of the location of the places. Below 3
different maps can be noticed:

\begin{itemize}
\tightlist
\item
  First we have a simple roadmap with the location as points.
\item
  Second, we have a density map of the points.
\item
  Third we have again a density map, but with the different density
  polygons shaped.
\end{itemize}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Maps1-1} \end{center}

 After creating the initial map, it is observed that some of the points
are concentrated in specific areas. By actually observing manually the
google maps, points of interest are defined which are also depicted and
are going to be used as part of the feature engineering process later
on.

These spots are:

\begin{itemize}
\tightlist
\item
  \textbf{Airport}\\
\item
  \textbf{IKEA}\\
\item
  \textbf{Fun Center}\\
\item
  \textbf{Municipal Airport}\\
\item
  \textbf{WalMart}\\
\item
  \textbf{University of Washington}\\
\item
  \textbf{Seattle College}\\
\item
  \textbf{Woodland Zoo}\\
\item
  \textbf{Lake}\\
\item
  \textbf{US football stadium}\\
\item
  \textbf{Discovery Park}\\
\item
  \textbf{Lincoln Park}\\
\item
  \textbf{Baseball Stadium}\\
\item
  \textbf{King Street Station}\\
\item
  \textbf{Sodo Station}\\
\item
  \textbf{Capitol Hill Station}\\
\item
  \textbf{Golf Course}
\item
  \textbf{Lake Wilderness}
\end{itemize}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Maps2-1} \end{center}

\hypertarget{feature-engineering-process}{%
\subsection{Feature Engineering
Process}\label{feature-engineering-process}}

Having a great overview of our dataset both from an intuitive, but also
from a machine learning perspective, it is time to move on to taking
care of our dataset.

Some of the obvious features that are going to be created are:

\begin{itemize}
\tightlist
\item
  \textbf{Month} out of the date.\\
\item
  \textbf{Year} out of the date.\\
\item
  \textbf{Age when sold} which comes out of year minus year built
\item
  \textbf{Age till today} which comes out of the year 2019 minus year
  built.
\end{itemize}

Furthermore, while looking at the description of the variables, it is
noticed that even though some houses might now have a year of
renovation, they might actually do. More specifically, if the values in
\textbf{sqft\_living15} and \textbf{sqft\_lot15} do not much the ones in
\textbf{sqft\_living} and \textbf{sqft\_lot}, it is implied that the
house has been renovated at some point. This new insight affects also
the variable \textbf{yr\_renovation}, as it is decided to be dropped
cause of many zero values (\emph{as seen in the plot below}) and be
replaced by a new one called again \textbf{yr\_renovated}, but actually
consisting of a binary variable on \emph{whether the house has been
under renovation or not}. The binary value is actually been calculated
by checking the difference between the variables mentioned imidiatelly
before.

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Transformations 2-1} \end{center}

As already discussed before, some variables are not just numerical, but
have a more deep meaning that classifies them as \textbf{ordinal} and
\textbf{factors}. These variables are the below and are transformed and
plotted in order to get a better overview of their behavior and
distribution across our dataset.

\begin{itemize}
\tightlist
\item
  \textbf{Grade}\\
\item
  \textbf{Condition}
\item
  \textbf{View}\\
\item
  \textbf{Year} and \textbf{Month}, as they are not useful as numeric
  anymore after the columns needed are created.
\end{itemize}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Factor 1-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Factor 2-1} \end{center}

Another thing that would be useful to dive into is the distribution of
\textbf{Year} along with the \textbf{Price}, but also the one between
\textbf{Month} with the \textbf{Price}.

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Year | Month | Price-1} \end{center}

As previously mentioned, while exploring the geospatial dimension of our
dataset, several possible hotspots were detected. At this time, we
calculate the distances of all the houses from all the hotspots. The
type of distance measurement method used, is the \textbf{haversine} that
is more accurate when calculating distances on a sphere. The coordinates
along with the point are shown in the table below.

name

long

lat

airport

-122.3

47.44

ikea

-122.2

47.44

fun\_center

-122.2

47.47

municipal\_airport

-122.2

47.49

walmart

-122.2

47.47

uni\_of\_washington

-122.3

47.66

seattle\_college

-122.3

47.70

woodland\_zoo

-122.4

47.67

lake

-122.3

47.68

us\_football\_stadium

-122.3

47.59

discovery\_park

-122.4

47.66

lincol\_park

-122.4

47.53

baseball\_stadium

-122.3

47.59

king\_street\_station

-122.3

47.60

sodo\_station

-122.3

47.58

capitol\_hill\_station

-122.3

47.62

golf\_course

-122.4

47.56

lake\_wilderness

-122.0

47.38

Now that we have the distance from all the hotspots, another next step
is to apply clustering to our houses. In order to cluster them, we are
going to use the \textbf{leaderCluster} package in which we have the
option to cluster points based on a \textbf{radius} that we define. This
approach makes more sense as we do not choose the amount of clusters
beforehand and we also use radius that is more reasonable for clustering
static points referring to the location of houses. More on that, the
algorithm gives us the \textbf{number of clusters} in which we do not
have any predefined influence (like in K-Means).

In order to determine the optimal number for the radius (in metres),
different values are tried and the below \emph{scatterplot} is shown.
Based on trial and error, along with some empirical rules, the
\textbf{knee} of the plot is chosen as the optimal to be used for the
clustering. In our case we choose \textbf{2200} as our radius.

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Knee for clusters-1} \end{center}

Some further variables that come to play are:

\begin{itemize}
\tightlist
\item
  \textbf{new}: whether the house is new or not. This is defined by
  checking if the \textbf{yr\_built} is the same as \textbf{year}.\\
\item
  \textbf{total\_area}: is pretty much described by the name itself and
  is calculated by the sum of \textbf{sqft\_living} and the
  \textbf{sqft\_basement}.
\end{itemize}

Now that we have all our variables that came up from the Feature
Engineering step. It is time to have an overview of our dataset once
more. Not only the \textbf{summary} and \textbf{structure} but also the
new \textbf{correlation matrix}.

\begin{verbatim}
## 'data.frame':    17277 obs. of  46 variables:
##  $ id                                : num  9.18e+09 4.64e+08 2.22e+09 6.16e+09 6.39e+09 ...
##  $ date                              : Date, format: "2014-05-13" "2014-08-27" ...
##  $ price                             : num  225000 641250 810000 330000 530000 ...
##  $ bedrooms                          : num  3 3 4 4 4 4 4 3 4 3 ...
##  $ bathrooms                         : num  1.5 2.5 3.5 1.5 1.75 3.5 3.25 2.25 2.5 1.5 ...
##  $ sqft_living                       : num  1250 2220 3980 1890 1814 ...
##  $ sqft_lot                          : num  7500 2550 209523 7540 5000 ...
##  $ floors                            : num  1 3 2 1 1 2 2 1 2 1 ...
##  $ waterfront                        : chr  "0" "0" "0" "0" ...
##  $ view                              : chr  "0" "2" "2" "0" ...
##  $ condition                         : chr  "3" "3" "3" "4" ...
##  $ grade                             : chr  "7" "10" "9" "7" ...
##  $ sqft_above                        : num  1250 2220 3980 1890 944 2480 4160 1130 2250 1500 ...
##  $ sqft_basement                     : num  0 0 0 0 870 640 0 310 0 1040 ...
##  $ yr_built                          : num  1967 1990 2006 1967 1951 ...
##  $ yr_renovated                      : chr  "YES" "YES" "YES" "YES" ...
##  $ zipcode                           : num  98030 98117 98024 98155 98115 ...
##  $ lat                               : num  47.4 47.7 47.6 47.8 47.7 ...
##  $ long                              : num  -122 -122 -122 -122 -122 ...
##  $ sqft_living15                     : num  1260 2200 2220 1890 1290 1880 3400 1510 2480 1870 ...
##  $ sqft_lot15                        : num  7563 5610 65775 8515 5000 ...
##  $ month                             : chr  "05" "08" "07" "01" ...
##  $ year                              : chr  "2014" "2014" "2014" "2015" ...
##  $ age_when_sold                     : num  47 24 8 48 63 6 19 31 27 55 ...
##  $ age                               : num  52 29 13 52 68 11 24 36 32 60 ...
##  $ distance_from_airport             : num  10.3 29 33.5 34.5 26.8 ...
##  $ distance_from_ikea                : num  7.91 30.85 28.5 35.27 27.18 ...
##  $ distance_from_fun_center          : num  11.1 27.7 28 32.1 24 ...
##  $ distance_from_municipal_airport   : num  12.9 26.8 25.6 30.6 22.4 ...
##  $ distance_from_walmart             : num  11.2 28.1 26.6 32.1 24 ...
##  $ distance_from_uni_of_washington   : num  32.74 8.04 32.84 10.45 2.99 ...
##  $ distance_from_seattle_college     : num  37.5 4.4 36.87 6.19 4.33 ...
##  $ distance_from_woodland_zoo        : num  34.64 4.36 36.76 9.69 5.49 ...
##  $ distance_from_lake                : num  35.24 4.56 36.24 8.64 4.43 ...
##  $ distance_from_us_football_stadium : num  26.3 12.2 33.4 17.7 10.6 ...
##  $ distance_from_discovery_park      : num  35.42 4.49 41.15 12.83 10.56 ...
##  $ distance_from_lincol_park         : num  22.3 18.5 38.2 25.6 19.2 ...
##  $ distance_from_baseball_stadium    : num  26 12.5 33.4 18.1 11 ...
##  $ distance_from_king_street_station : num  26.7 11.9 33.4 17.3 10.2 ...
##  $ distance_from_sodo_station        : num  24.8 13.7 33 19.2 12 ...
##  $ distance_from_capitol_hill_station: num  28.62 10.2 33.02 14.97 7.82 ...
##  $ distance_from_golf_course         : num  24 15.2 36 21.9 15.3 ...
##  $ distance_from_lake_wilderness     : num  13.5 44.7 23 47.1 39 ...
##  $ cluster                           : int  1 2 3 4 5 5 6 7 8 9 ...
##  $ new                               : chr  "NO" "NO" "NO" "NO" ...
##  $ total_area                        : num  1250 2220 3980 1890 2684 ...
\end{verbatim}

id

date

price

bedrooms

bathrooms

sqft\_living

sqft\_lot

floors

waterfront

view

condition

grade

sqft\_above

sqft\_basement

yr\_built

yr\_renovated

zipcode

lat

long

sqft\_living15

sqft\_lot15

month

year

age\_when\_sold

age

distance\_from\_airport

distance\_from\_ikea

distance\_from\_fun\_center

distance\_from\_municipal\_airport

distance\_from\_walmart

distance\_from\_uni\_of\_washington

distance\_from\_seattle\_college

distance\_from\_woodland\_zoo

distance\_from\_lake

distance\_from\_us\_football\_stadium

distance\_from\_discovery\_park

distance\_from\_lincol\_park

distance\_from\_baseball\_stadium

distance\_from\_king\_street\_station

distance\_from\_sodo\_station

distance\_from\_capitol\_hill\_station

distance\_from\_golf\_course

distance\_from\_lake\_wilderness

cluster

new

total\_area

Min. :1.00e+06

Min. :2014-05-02

Min. : 78000

Min. : 1.00

Min. :0.50

Min. : 370

Min. : 520

Min. :1.00

Length:17277

Length:17277

Length:17277

Length:17277

Min. : 370

Min. : 0

Min. :1900

Length:17277

Min. :98001

Min. :47.2

Min. :-123

Min. : 460

Min. : 659

Length:17277

Length:17277

Min. : -1.0

Min. : 4

Min. : 0.98

Min. : 1.3

Min. : 1.22

Min. : 0.32

Min. : 0.66

Min. : 0.67

Min. : 0.27

Min. : 0.32

Min. : 0.54

Min. : 1.25

Min. : 0.68

Min. : 0.38

Min. : 1.19

Min. : 1.17

Min. : 0.93

Min. : 0.09

Min. : 0.37

Min. : 0.33

Min. : 1

Length:17277

Min. : 370

1st Qu.:2.11e+09

1st Qu.:2014-07-21

1st Qu.: 320000

1st Qu.: 3.00

1st Qu.:1.75

1st Qu.: 1430

1st Qu.: 5050

1st Qu.:1.00

Class :character

Class :character

Class :character

Class :character

1st Qu.:1190

1st Qu.: 0

1st Qu.:1951

Class :character

1st Qu.:98033

1st Qu.:47.5

1st Qu.:-122

1st Qu.:1490

1st Qu.: 5100

Class :character

Class :character

1st Qu.: 18.0

1st Qu.: 22

1st Qu.:13.39

1st Qu.:13.4

1st Qu.:12.49

1st Qu.:12.36

1st Qu.:12.66

1st Qu.: 9.18

1st Qu.: 9.74

1st Qu.:10.76

1st Qu.:10.25

1st Qu.:10.22

1st Qu.:12.55

1st Qu.:14.67

1st Qu.:10.38

1st Qu.:10.07

1st Qu.:10.76

1st Qu.: 9.59

1st Qu.:12.68

1st Qu.:20.48

1st Qu.:106

Class :character

1st Qu.: 1500

Median :3.90e+09

Median :2014-10-16

Median : 450000

Median : 3.00

Median :2.25

Median : 1910

Median : 7620

Median :1.50

Mode :character

Mode :character

Mode :character

Mode :character

Median :1564

Median : 0

Median :1975

Mode :character

Median :98065

Median :47.6

Median :-122

Median :1840

Median : 7639

Mode :character

Mode :character

Median : 39.0

Median : 44

Median :21.77

Median :20.1

Median :19.78

Median :18.97

Median :19.19

Median :16.15

Median :19.91

Median :18.64

Median :18.55

Median :16.71

Median :21.87

Median :21.33

Median :16.82

Median :16.58

Median :16.88

Median :15.92

Median :19.33

Median :28.45

Median :216

Mode :character

Median : 2170

Mean :4.57e+09

Mean :2014-10-28

Mean : 539865

Mean : 3.37

Mean :2.11

Mean : 2080

Mean : 15186

Mean :1.49

NA

NA

NA

NA

Mean :1791

Mean : 289

Mean :1971

NA

Mean :98078

Mean :47.6

Mean :-122

Mean :1986

Mean : 12826

NA

NA

Mean : 43.3

Mean : 48

Mean :21.64

Mean :20.8

Mean :19.46

Mean :18.65

Mean :19.30

Mean :19.16

Mean :21.82

Mean :20.77

Mean :20.76

Mean :18.41

Mean :23.23

Mean :21.31

Mean :18.42

Mean :18.40

Mean :18.23

Mean :18.40

Mean :19.72

Mean :28.52

Mean :237

NA

Mean : 2369

3rd Qu.:7.30e+09

3rd Qu.:2015-02-17

3rd Qu.: 645500

3rd Qu.: 4.00

3rd Qu.:2.50

3rd Qu.: 2550

3rd Qu.: 10695

3rd Qu.:2.00

NA

NA

NA

NA

3rd Qu.:2210

3rd Qu.: 556

3rd Qu.:1997

NA

3rd Qu.:98117

3rd Qu.:47.7

3rd Qu.:-122

3rd Qu.:2360

3rd Qu.: 10080

NA

NA

3rd Qu.: 63.0

3rd Qu.: 68

3rd Qu.:28.76

3rd Qu.:28.7

3rd Qu.:25.96

3rd Qu.:24.76

3rd Qu.:25.92

3rd Qu.:26.68

3rd Qu.:31.07

3rd Qu.:29.66

3rd Qu.:29.55

3rd Qu.:25.04

3rd Qu.:32.53

3rd Qu.:27.63

3rd Qu.:24.89

3rd Qu.:25.10

3rd Qu.:24.48

3rd Qu.:25.21

3rd Qu.:26.22

3rd Qu.:39.04

3rd Qu.:351

NA

3rd Qu.: 2980

Max. :9.90e+09

Max. :2015-05-27

Max. :7700000

Max. :33.00

Max. :8.00

Max. :13540

Max. :1164794

Max. :3.50

NA

NA

NA

NA

Max. :9410

Max. :4820

Max. :2015

NA

Max. :98199

Max. :47.8

Max. :-121

Max. :6210

Max. :871200

NA

NA

Max. :115.0

Max. :119

Max. :79.93

Max. :74.9

Max. :74.52

Max. :72.07

Max. :73.11

Max. :74.71

Max. :79.64

Max. :78.01

Max. :78.06

Max. :77.35

Max. :82.80

Max. :83.67

Max. :77.50

Max. :77.18

Max. :77.33

Max. :76.08

Max. :80.93

Max. :65.91

Max. :634

NA

Max. :17670

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Correlation 2-1} \end{center}

Now that we have all the necessary content, it is time to convert it to
the appropriate format. This means:

\begin{itemize}
\tightlist
\item
  \textbf{One hot encoding} of the factor variables\\
\item
  \textbf{Scaling numerical variables} from 0 to 1\\
\item
  Applying the \textbf{logarithmic} transformation on the price to
  remove the effect that the right skewed distribution might have to our
  predictive models.
\end{itemize}

\hypertarget{machine-learning-pipeline}{%
\subsection{Machine Learning Pipeline}\label{machine-learning-pipeline}}

\hypertarget{dealing-with-multicollinearity-applying-lasso-regression-model}{%
\subsubsection{Dealing with Multicollinearity \textbar{} Applying Lasso
Regression
Model}\label{dealing-with-multicollinearity-applying-lasso-regression-model}}

One of the first things we need to do in order to make sure the models
do not \textbf{overfit}, is to deal with the issue of
\emph{multicollinearity} that is obviously appearing in our dataset,
just by looking at the correlation matrix immediately above. To find the
perfect set of variables, a \textbf{lasso regression} model is created
that is not only trained to predict, but is able to eliminate
unnecessary variables. To train the model a \textbf{grid search} is
applied to find the optimal set of parameters. The variables and the
model are shown below, along with the new correlation amtrix with the
variables kept.\\
\emph{(the predictions are stored in the \textbf{predictions\_lasso}
variable)}

\begin{verbatim}
## Lasso uses 48 variables in its model, and did not select 28 variables.
\end{verbatim}

\begin{verbatim}
## glmnet 
## 
## 17277 samples
##    48 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13822, 13822, 13821, 13822, 13821 
## Resampling results:
## 
##   RMSE    Rsquared  MAE   
##   0.1983  0.8591    0.1486
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## 
## Tuning parameter 'lambda' was held constant at a value of 0.001
\end{verbatim}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/Correlation 3-1} \end{center}

\hypertarget{applying-recursive-feature-elimination-random-forest-modelling-prediction}{%
\subsubsection{Applying Recursive Feature Elimination \textbar{} Random
Forest modelling \&
prediction}\label{applying-recursive-feature-elimination-random-forest-modelling-prediction}}

Apart from just the set of variables that lasso regression comes up
with, a \textbf{Recursive Feature Elimination (RFE)} is done to our full
dataset, in order to both determine the optimal variables, but also the
set of parameters for a random forest algorithm (which is the algorithm
used from RFE variable selection).\\
In order to cover all different possiblities, another \textbf{RFE
approach} is followed apart from the one on the \textbf{whole dataset}.
The other one is performing an \textbf{RFE analysis} on only the
variables that came out as important from the \textbf{Lasso regression
elimination.}\\
The results are shown below and the plots are of 2 kinds. :

\begin{itemize}
\tightlist
\item
  The first plot shows the performance of the model based on
  \textbf{RMSE} metric, along with the \textbf{min.node.size}
  parameter.\\
\item
  The second plot shows the \textbf{variable importance} that comes out
  of the RFE method.
\end{itemize}

\begin{verbatim}
## Random Forest 
## 
## 17277 samples
##    76 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13822, 13821, 13823, 13821, 13821 
## Resampling results across tuning parameters:
## 
##   mtry  min.node.size  RMSE    Rsquared  MAE   
##    2     5             0.1913  0.8902    0.1345
##    2    10             0.1925  0.8894    0.1355
##    2    15             0.1943  0.8882    0.1367
##    2    20             0.1953  0.8871    0.1374
##    3     5             0.1755  0.8962    0.1221
##    3    10             0.1764  0.8958    0.1228
##    3    15             0.1777  0.8947    0.1238
##    3    20             0.1790  0.8938    0.1249
##    4     5             0.1726  0.8972    0.1199
##    4    10             0.1733  0.8967    0.1205
##    4    15             0.1743  0.8961    0.1214
##    4    20             0.1755  0.8951    0.1224
##    5     5             0.1718  0.8971    0.1195
##    5    10             0.1724  0.8967    0.1200
##    5    15             0.1732  0.8962    0.1207
##    5    20             0.1742  0.8954    0.1216
##    6     5             0.1715  0.8969    0.1194
##    6    10             0.1719  0.8967    0.1199
##    6    15             0.1729  0.8958    0.1206
##    6    20             0.1735  0.8954    0.1213
##    7     5             0.1714  0.8966    0.1195
##    7    10             0.1719  0.8963    0.1200
##    7    15             0.1726  0.8957    0.1206
##    7    20             0.1733  0.8951    0.1212
##    8     5             0.1717  0.8961    0.1197
##    8    10             0.1721  0.8958    0.1202
##    8    15             0.1726  0.8954    0.1207
##    8    20             0.1733  0.8948    0.1214
##    9     5             0.1718  0.8957    0.1199
##    9    10             0.1722  0.8954    0.1204
##    9    15             0.1727  0.8950    0.1209
##    9    20             0.1733  0.8945    0.1215
##   10     5             0.1719  0.8954    0.1201
##   10    10             0.1721  0.8953    0.1204
##   10    15             0.1728  0.8946    0.1211
##   10    20             0.1735  0.8940    0.1217
##   11     5             0.1721  0.8951    0.1203
##   11    10             0.1725  0.8947    0.1208
##   11    15             0.1728  0.8945    0.1211
##   11    20             0.1735  0.8938    0.1218
##   12     5             0.1723  0.8947    0.1205
##   12    10             0.1726  0.8945    0.1209
##   12    15             0.1732  0.8939    0.1214
##   12    20             0.1737  0.8935    0.1219
## 
## Tuning parameter 'splitrule' was held constant at a value of variance
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 7, splitrule =
##  variance and min.node.size = 5.
\end{verbatim}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/RFE 1-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/RFE 2-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/RFE lasso 1-1} \end{center}

\begin{center}\includegraphics{Ind_Proj_Stavros_Tsentemeidis_files/figure-latex/RFE lasso 2-1} \end{center}

Based on the results shown above the optimal set of parameters is:

\begin{itemize}
\tightlist
\item
  \textbf{mtry} = 7\\
\item
  \textbf{splitrule} = variance\\
\item
  \textbf{min.node.size} = 5.
\end{itemize}

Furthermore, the number of variables that seem to be important are
\textbf{46} on the one case and \textbf{26} on the lasso variables set
and are depicted on the following table:

Importance RFE

lat

100.0000

sqft\_living

78.6635

distance\_from\_seattle\_college

57.8079

distance\_from\_uni\_of\_washington

57.2729

distance\_from\_airport

54.5906

sqft\_above

50.4365

total\_area

45.7572

distance\_from\_ikea

45.2592

sqft\_living15

45.1625

distance\_from\_capitol\_hill\_station

44.2494

distance\_from\_lake

43.4481

distance\_from\_woodland\_zoo

41.8831

distance\_from\_discovery\_park

34.9077

distance\_from\_lake\_wilderness

34.3699

distance\_from\_king\_street\_station

32.6618

distance\_from\_us\_football\_stadium

32.5490

distance\_from\_fun\_center

31.6622

distance\_from\_walmart

28.8149

distance\_from\_baseball\_stadium

28.7250

distance\_from\_sodo\_station

27.3781

distance\_from\_lincol\_park

27.2188

distance\_from\_golf\_course

25.6889

long

25.6295

distance\_from\_municipal\_airport

22.5743

grade\_7

17.8855

bathrooms

16.8208

zipcode

16.3992

sqft\_lot15

13.9909

sqft\_lot

13.5837

yr\_built

10.3284

age\_when\_sold

10.1419

age

9.8951

grade\_9

8.9069

grade\_8

8.3342

view\_0

6.5668

floors

6.2550

grade\_6

5.7499

sqft\_basement

5.6292

grade\_10

5.6188

bedrooms

3.5006

grade\_11

2.5815

cluster

2.3758

view\_4

2.2699

waterfront\_1

1.8324

waterfront\_0

1.6384

condition\_3

1.0375

grade\_12

0.4120

condition\_4

0.3897

view\_3

0.3316

grade\_5

0.3094

view\_2

0.2792

condition\_5

0.2721

year\_2015

0.2580

year\_2014

0.2308

month\_04

0.1484

new\_NO

0.0922

view\_1

0.0859

new\_YES

0.0811

condition\_2

0.0778

grade\_13

0.0753

month\_12

0.0558

grade\_4

0.0528

month\_03

0.0479

month\_09

0.0446

month\_07

0.0442

month\_01

0.0405

grade\_3

0.0400

yr\_renovated\_YES

0.0390

month\_05

0.0327

month\_11

0.0326

month\_08

0.0298

yr\_renovated\_NO

0.0266

month\_06

0.0256

month\_10

0.0234

month\_02

0.0228

condition\_1

0.0000

Importance RFE Lasso

lat

100.0000

sqft\_living

76.9222

distance\_from\_uni\_of\_washington

61.8815

distance\_from\_airport

49.1569

distance\_from\_us\_football\_stadium

47.0836

distance\_from\_ikea

46.6802

distance\_from\_discovery\_park

45.5734

distance\_from\_lake\_wilderness

40.7769

distance\_from\_sodo\_station

40.1683

sqft\_living15

39.5668

total\_area

38.5610

distance\_from\_walmart

29.8270

long

27.2526

grade\_7

20.9499

bathrooms

17.1336

zipcode

16.7408

sqft\_lot

13.8408

sqft\_lot15

13.6446

yr\_built

13.6221

grade\_8

8.3932

floors

7.3500

grade\_6

6.1773

grade\_10

5.7172

bedrooms

4.4644

grade\_11

3.5034

view\_4

2.7963

waterfront\_1

2.1995

condition\_4

0.7583

grade\_12

0.7361

view\_3

0.6028

condition\_5

0.4979

view\_2

0.4620

grade\_5

0.3845

year\_2015

0.3333

new\_NO

0.1689

month\_04

0.1669

view\_1

0.1665

grade\_4

0.0667

condition\_2

0.0661

month\_07

0.0561

grade\_3

0.0423

month\_03

0.0403

month\_11

0.0357

month\_05

0.0290

month\_09

0.0205

condition\_1

0.0121

month\_10

0.0000

At this stage, it is decided to train a \textbf{RandomForest}, based on
the parameters and variables found below. The results of the model are
shown below.

(the predictions are stored in the \textbf{predictions\_rf} and
\textbf{predictions\_rf\_lasso} variable)

\hypertarget{random-forest-with-rfe}{%
\paragraph{Random Forest with RFE}\label{random-forest-with-rfe}}

\begin{verbatim}
## Random Forest 
## 
## 17277 samples
##    42 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13822, 13822, 13821, 13821, 13822 
## Resampling results:
## 
##   RMSE   Rsquared  MAE   
##   0.174  0.8924    0.1217
## 
## Tuning parameter 'mtry' was held constant at a value of 7
## Tuning
##  parameter 'splitrule' was held constant at a value of variance
## 
## Tuning parameter 'min.node.size' was held constant at a value of 5
\end{verbatim}

\hypertarget{random-forest-with-rfelasso}{%
\paragraph{Random Forest with
RFE(lasso)}\label{random-forest-with-rfelasso}}

\begin{verbatim}
## Random Forest 
## 
## 17277 samples
##    26 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13822, 13822, 13821, 13821, 13822 
## Resampling results:
## 
##   RMSE    Rsquared  MAE   
##   0.1794  0.8851    0.1256
## 
## Tuning parameter 'mtry' was held constant at a value of 7
## Tuning
##  parameter 'splitrule' was held constant at a value of variance
## 
## Tuning parameter 'min.node.size' was held constant at a value of 5
\end{verbatim}

\hypertarget{xgboost-modelling-prediction}{%
\subsubsection{XGBoost Modelling \&
Prediction}\label{xgboost-modelling-prediction}}

Another algorithm that is proved really strong perfomer, is the
\textbf{XGBoost} one (Extreme Gradient Boosting). The algorithm is
trained on the \textbf{RFE features} both of lasso and of the full train
set and an extensive \textbf{grid search} is applied, in order to
determine the optimal set of parameters. The results are show below.

(the predictions are stored in the \textbf{predictions\_xgb} and
\textbf{predictions\_xgb\_lasso} variable)

\hypertarget{extreme-gradient-boosting-with-rfe}{%
\paragraph{Extreme Gradient Boosting with
RFE}\label{extreme-gradient-boosting-with-rfe}}

\begin{verbatim}
## eXtreme Gradient Boosting 
## 
## 17277 samples
##    43 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13822, 13823, 13822, 13820, 13821 
## Resampling results across tuning parameters:
## 
##   eta   max_depth  min_child_weight  RMSE    Rsquared  MAE   
##   0.01  2          1                 0.1984  0.8604    0.1458
##   0.01  2          2                 0.1984  0.8604    0.1457
##   0.01  2          3                 0.1984  0.8604    0.1457
##   0.01  2          4                 0.1984  0.8604    0.1457
##   0.01  2          5                 0.1984  0.8604    0.1457
##   0.01  3          1                 0.1835  0.8796    0.1336
##   0.01  3          2                 0.1836  0.8796    0.1336
##   0.01  3          3                 0.1835  0.8797    0.1336
##   0.01  3          4                 0.1835  0.8797    0.1336
##   0.01  3          5                 0.1835  0.8797    0.1336
##   0.01  4          1                 0.1764  0.8885    0.1277
##   0.01  4          2                 0.1764  0.8885    0.1277
##   0.01  4          3                 0.1763  0.8886    0.1276
##   0.01  4          4                 0.1762  0.8888    0.1275
##   0.01  4          5                 0.1762  0.8887    0.1276
##   0.01  5          1                 0.1724  0.8934    0.1239
##   0.01  5          2                 0.1724  0.8934    0.1239
##   0.01  5          3                 0.1722  0.8936    0.1239
##   0.01  5          4                 0.1722  0.8936    0.1239
##   0.01  5          5                 0.1721  0.8938    0.1239
##   0.01  6          1                 0.1699  0.8965    0.1215
##   0.01  6          2                 0.1702  0.8961    0.1216
##   0.01  6          3                 0.1699  0.8965    0.1215
##   0.01  6          4                 0.1698  0.8966    0.1215
##   0.01  6          5                 0.1696  0.8968    0.1214
##   0.05  2          1                 0.1733  0.8923    0.1248
##   0.05  2          2                 0.1733  0.8923    0.1248
##   0.05  2          3                 0.1732  0.8924    0.1248
##   0.05  2          4                 0.1731  0.8925    0.1247
##   0.05  2          5                 0.1731  0.8925    0.1247
##   0.05  3          1                 0.1688  0.8977    0.1207
##   0.05  3          2                 0.1686  0.8980    0.1206
##   0.05  3          3                 0.1686  0.8979    0.1206
##   0.05  3          4                 0.1685  0.8980    0.1205
##   0.05  3          5                 0.1684  0.8982    0.1204
##   0.05  4          1                 0.1673  0.8995    0.1191
##   0.05  4          2                 0.1673  0.8995    0.1189
##   0.05  4          3                 0.1673  0.8995    0.1190
##   0.05  4          4                 0.1669  0.9000    0.1187
##   0.05  4          5                 0.1670  0.8999    0.1188
##   0.05  5          1                 0.1671  0.8999    0.1183
##   0.05  5          2                 0.1671  0.8998    0.1184
##   0.05  5          3                 0.1668  0.9001    0.1185
##   0.05  5          4                 0.1670  0.8999    0.1184
##   0.05  5          5                 0.1671  0.8998    0.1184
##   0.05  6          1                 0.1679  0.8988    0.1185
##   0.05  6          2                 0.1674  0.8994    0.1182
##   0.05  6          3                 0.1676  0.8992    0.1183
##   0.05  6          4                 0.1674  0.8995    0.1183
##   0.05  6          5                 0.1675  0.8993    0.1182
##   0.10  2          1                 0.1698  0.8965    0.1217
##   0.10  2          2                 0.1697  0.8966    0.1216
##   0.10  2          3                 0.1696  0.8968    0.1216
##   0.10  2          4                 0.1694  0.8970    0.1215
##   0.10  2          5                 0.1695  0.8968    0.1215
##   0.10  3          1                 0.1676  0.8992    0.1194
##   0.10  3          2                 0.1677  0.8990    0.1194
##   0.10  3          3                 0.1678  0.8990    0.1195
##   0.10  3          4                 0.1677  0.8991    0.1194
##   0.10  3          5                 0.1678  0.8989    0.1195
##   0.10  4          1                 0.1682  0.8986    0.1194
##   0.10  4          2                 0.1685  0.8982    0.1194
##   0.10  4          3                 0.1689  0.8977    0.1198
##   0.10  4          4                 0.1684  0.8983    0.1194
##   0.10  4          5                 0.1684  0.8982    0.1193
##   0.10  5          1                 0.1698  0.8966    0.1197
##   0.10  5          2                 0.1698  0.8966    0.1199
##   0.10  5          3                 0.1693  0.8972    0.1198
##   0.10  5          4                 0.1698  0.8966    0.1199
##   0.10  5          5                 0.1701  0.8963    0.1200
##   0.10  6          1                 0.1710  0.8952    0.1207
##   0.10  6          2                 0.1706  0.8956    0.1201
##   0.10  6          3                 0.1711  0.8951    0.1207
##   0.10  6          4                 0.1709  0.8953    0.1203
##   0.10  6          5                 0.1709  0.8953    0.1205
## 
## Tuning parameter 'nrounds' was held constant at a value of 1000
## 
## Tuning parameter 'colsample_bytree' was held constant at a value of
##  1
## Tuning parameter 'subsample' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were nrounds = 1000, max_depth =
##  5, eta = 0.05, gamma = 0, colsample_bytree = 1, min_child_weight = 3
##  and subsample = 1.
\end{verbatim}

\hypertarget{extreme-gradient-boosting-with-rfelasso}{%
\paragraph{Extreme Gradient Boosting with
RFE(lasso)}\label{extreme-gradient-boosting-with-rfelasso}}

\begin{verbatim}
## eXtreme Gradient Boosting 
## 
## 17277 samples
##    26 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13821, 13821, 13822, 13821, 13823 
## Resampling results:
## 
##   RMSE   Rsquared  MAE   
##   0.169  0.8975    0.1199
## 
## Tuning parameter 'nrounds' was held constant at a value of 1000
##  1
## Tuning parameter 'min_child_weight' was held constant at a value of
##  3
## Tuning parameter 'subsample' was held constant at a value of 1
\end{verbatim}

\hypertarget{final-metrics-of-all-the-algorithms-to-choose}{%
\subsubsection{Final Metrics of all the algorithms to
choose}\label{final-metrics-of-all-the-algorithms-to-choose}}

In order to have a better view of our true algorithms performance, we
test models on the \textbf{hold out set}. At this step i would like to
re assure you that the test set \textbf{has not been used} at any step
of the training procedure apart from this stage just for verification
purposes. The script that reads the file and extracts the hold out set
is the \textbf{final\_metrics.R} one and is read only at this point in
time.

Moreover, apart from the individual model predictions, it is decided to
stack the predictions of the best 3 models using a \textbf{simple AVG}
and a \textbf{weighted AVG}. The reason behind this choice is the fact
that, by having a look at the predictions of the different models, it
can be noticed that \emph{lasso} along with \emph{ranger} and
\emph{xgboost} are pretty much uncorrelated. This lead us to the
conclusion that they capture different aspects of the test set. As a
result an average could possibly get the best out of the three
approaches.

Minimum RMSE through 10-fold Cross Validation

LASSO

XGB

XGB\_LASSO

RF

RF\_LASSO

0.1983

0.1668

0.169

0.174

0.1794

Overall Metrics for the models

RMSE

MAE

MAPE

LASSO

200091

136489

0.2388

RF

165372

110364

0.2130

RF\_LASSO

175149

112263

0.2152

XGB

142374

80158

0.1448

XGB\_LASSO

139424

82463

0.1530

AVG\_RF\_XGB\_LASSO

118293

70724

0.1295

W\_AVG\_RF\_XGB\_LASSO

121988

71976

0.1305

\hypertarget{predictions-on-the-hold-out-set-submission}{%
\subsubsection{Predictions on the hold-out set \&
Submission}\label{predictions-on-the-hold-out-set-submission}}

Based on the results of the different models on Cross Validation, we
would choose the XGBoost which seems to perform better. By having a look
at the predictions and performance on the actual hold out set we verify
this assumption that XGboost would be the model to go, However, we
choose the \textbf{Averaged Predictions of our 3 best models XGB, RF and
LASSO} as it manages to do what we thought it would, which is capturing
more and more of the variance of the hold out set by far better than a
simple algorithm. After that we create the submission file that has 2
fields: \textbf{id} of the house, plus the predicted \textbf{price}.

\begin{verbatim}
## The following objects are masked from submission_stavros_tsentemeidis (pos = 3):
## 
##     id, price
\end{verbatim}

\begin{verbatim}
## The following objects are masked from submission_stavros_tsentemeidis (pos = 4):
## 
##     id, price
\end{verbatim}

Submission File

id

price

5200087

298745

11500240

536759

11520200

244232

13001215

377221

13001795

420009

13001991

434331

\hypertarget{conclusions}{%
\subsection{Conclusions}\label{conclusions}}

As next steps, some of which were tried but not to the full extent to
the pipeline would be:

\begin{itemize}
\tightlist
\item
  Stacking models in order to improve predictions.(caretStack was
  creating errors for regression, you can find the script named as
  \emph{stacking.R})\\
\item
  Applying neural networks in order to try improving the performance.(an
  initial approach was followed but needed a lot more optimization)\\
\item
  Think of further external variables as Feature Engineering that would
  give more insights on the data. For example, maybe driving distance
  from hotspots would be more interpretable than just the haversine one
  (could try but google needed paying fee)
\end{itemize}

\emph{Stavros Tsentemeidis} \textbar{} \emph{Advanced R} \textbar{}
\emph{Individual Assignment} \textbar{} \emph{House Prices Seattle}

\emph{Master in Big Data and Business Analytics \textbar{} Oct 2018
Intake \textbar{} IE School of Human Sciences and Technology}


\end{document}
